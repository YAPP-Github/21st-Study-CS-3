# 9.1 크롤러와 크롤링

## 1. 개요

- `웹 크롤러`
  - 먼저 웹 페이지를 한 개 가져오고 그 다음 그 페이지가 가리키는 모든 웹페이지를 가져오고 다시 그 페이지들이 가리키는 모든 웹페이지들을 가져오는, 이러한 일을 `재귀적으로 반복`하는 방식으로 `웹을 순회하는 로봇`
  - 웹 링크를 재귀적으로 따라가는 로봇을 `크롤러` 혹은 `스파이더`라고 부름
  - HTML 하이퍼링크들로 만들어진 웹을 따라 `crawl(기어다니기)` 때문
- 인터넷 검색엔진은 웹을 돌아다니면서 그들이 만나는 모든 문서를 끌어오기 위해 크롤러를 사용
- 이 문서들을 나중에 `검색가능한 데이터베이스로 만들어지는데 이는 사용자들이 특정 단어를 포함한 문서를 찾을 수 있게 해줌`

## 2. 어디에서 시작하는가: ‘루트 집합’(9.1.1)

- 크롤러를 풀어놓기 전에 우선 `출발지점을 주어야 함`
- 크롤러가 방문을 시작하는 URL들의 초기 집합은 `루트집합(root set)`
- 루트집합을 고를 때 모든 링크를 크롤링하면 결과적으로 관심 있는 웹페이지들의 대부분을 가져오게 될 수 있도록 충분히 다른 장소에서 URL들을 선택해야 한다
- 하지만 진짜 웹에서는 `최종적으로 모든 문서로 이어지게 되는 하나의 문서란 없다`

  ![9-1.jpg](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/d4caf77f-d464-4db5-9650-239e54c98478/9-1.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230205%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230205T144240Z&X-Amz-Expires=86400&X-Amz-Signature=acef66d4c21a963b76f1800cbeef876c5aa6e1d0311c76126b13f845ca4bdc35&X-Amz-SignedHeaders=host&response-content-disposition=filename%3D%229-1.jpg%22&x-id=GetObject)

- 사진에서, 만약 문서 A에서 시작한다면 B, C, D를 얻고 거기서 E,F를 얻은 뒤 J를 얻고 마지막으로 K을 얻음
  - 그러나 `A에서 G로, A에서 N으로 이어지는 연결고리는 없음`
- 웹의 몇몇 웹 페이지들은 `S, T, U 와 같이 그들로 향하는 어떤 링크도 없이 오도 가도 못하게 거의 고립`되어 있음
  - 외로운 페이지들은 아마도 새로이 만들어지고 나서 아직 누구도 찾아내지 못한 것들일 것
  - 혹은 정말 오래되었거나 잘 알려져있지 않은 것
- 일반적으로 웹의 대부분을 커버하기 위해 루트 집합에 너무 많은 페이지가 있을 필요는 없음
- 사진에서, 오직 `A, G, S`가 루트 집합에 있기만 하면 모든 페이지에 도달할 수 있음
- 일반적으로 `좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생선된 페이지들의 목록, 자주 링크되지 않는 잘 알려져있지 않은 페이지들의 목록으로 구성`

## 3. 링크 추출과 상대 링크 정상화(9.1.2)

- 크롤러는 검색한 각 페이지 안에 들어있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가해야 한다
- 크롤러가 크롤링을 진행하면서 탐색해야 할 새 링크를 발견함에 따라 이 목록은 급속히 확장된다
- 크롤러들은 간단한 HTML 파싱을 해서 이들 링크들을 추출하고 상대 링크를 절대 링크로 변환할 필요가 있다

## 4. 순환 피하기(9.1.3)

- 로봇이 웹을 크롤링할 때 `루프나 순환에 빠지지 않도록` 매우 조심해야 한다

  ![KakaoTalk_20230205_213639205.jpg](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/6791224a-77fa-4d00-9088-9a291fa3e68f/KakaoTalk_20230205_213639205.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230205%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230205T144314Z&X-Amz-Expires=86400&X-Amz-Signature=7e12ad385abed7cc627ef127617d86da37ea4ca88f8983b72eb546f99bd40784&X-Amz-SignedHeaders=host&response-content-disposition=filename%3D%22KakaoTalk_20230205_213639205.jpg%22&x-id=GetObject)

- 그림에서 로봇은 `페이지 A를 가져와서 B가 A에 링크되어 있는 것을 보고 B를 가져온다`
- 로봇은 페이지 `B를 가져와서 C가 B에 링크되어 있는 것을 보고 C를 가져온다`
- 로봇은 페이지 C를 가져와서 A가 C에 링크되어 있는 것을 본다. 만약 로봇이 A를 다시 가져온다면 `A, B, C, A, B, C, A…`를 반복하는 순환에 빠지게 된다
- 즉, 로봇들은 순환을 피하기 위해 반드시 그들이 어디를 방문했는지 알아야 함
- 순환은 로봇을 함정에 빠뜨려서 멈추게 하거나 진행을 느려지게 한다

## 5. 루프와 중복(9.1.4)

- 순환은 다음의 세가지 이유로 크롤러에게 해롭다
  - `순환은 크롤러를 루프에 빠뜨려서 꼼짝 못하게 만들 수 있다.`
    - 루프는 허술하게 설계된 크롤러를 빙빙 돌게 만들 수 있고 같은 페이지들을 반복해서 가져오는데 모든 시간을 허비하게 만들 수 있다
    - 크롤러가 네트워크 대역폭을 다 차지하고 그 어떤 페이지도 가져올 수 없게 되어버릴 수 있다
  - `크롤러가 같은 페이지를 반복해서 가져오면 고스란히 웹 서버의 부담이 됨`
    - 만약 크롤러의 네트워크 접근 속도가 충분히 빠르다면 웹 사이트를 압박하여 어떤 실제 사용자도 사이트에 접근할 수 없도록 막아버리게 될 수도 있다
    - 이러한 서비스 방해행위는 법적인 문제제기의 근거가 될 수도 있다
  - `비록 루프 자체가 문제가 되지 않더라도 크롤러는 많은 수의 중복된 페이지(loops 와 운율을 맞추기 위해 dups 라고 불리는) 들을 가져오게 된다`
    - 크롤러의 애플리케이션은 자신을 쓸모없게 만드는 중복된 콘텐츠로 넘쳐나게 될 것이다
    - 수백개의 정확히 똑같은 페이지를 반환하는 인터넷 검색엔진이 그 예다

## 6. 빵 부스러기의 흔적(9.1.5)

- 불행히도 방문한 곳을 지속적으로 추적하는 것은 쉽지 않다
- 만약 전 세계 웹 콘텐츠의 상당 부분을 크롤링하려 한다면 `수십억 개의 URL을 방문할 준비가 필요한 것`
  - 어떤 URL을 방문하였는지 계속 추적하는 작업은 꽤나 도전적인 일일 수 있다
  - URL들은 굉장히 많기 떄문에 어떤 URL을 방문했는지 빠르게 판단하기 위해서는 `복잡한 자료 구조`를 사용할 필요가 있다
  - 자료구조는 속도와 메모리 사용 면에서 효과적이어야 한다
- 수억개의 URL은 빠른 검색구조를 요구하기 때문에 `빠른 속도` 중요
- URL목록의 완벽한 검색은 불가능
- 로봇은 어떤 URL이 방문했던 곳인지 빠르게 결정하기위해 적어도 검색 트리나 해시 테이블을 필요로 함
- 수억 개의 URL은 많은 공간을 차지
- 대규모 웹 크롤러가 그들이 방문한 곳을 관리하기 위해 사용하는 유용한 기법을 몇가지 들어보면 다음과 같다
  - `트리와 해시 테이블`
    - 복잡한 로봇이라면 방문한 URL을 추적하기 위해 검색 트리나 해시 테이블을 사용했을 수도 있다
    - URL을 훨씬 빨리 찾아볼 수 있게 해주는 소프트웨어 자료구조
  - `느슨한 존재 비트맵`
    - 공간 사용을 최소화하기 위해 몇몇 대규모 크롤러들은 존재 비트 배열과 같은, 느슨한 자료구조를 사용
    - 각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는 존재비트를 갖는다
    - URL이 크롤링되었을 때 해당하는 존재비트가 만들어진다
    - 만약 존재 비트가 이미 존재한다면 크롤러는 그 URL을 이미 크롤링되었다고 간주
  - `체크포인트`
    - 로봇 프로그램이 갑작스럽게 중단될 경우를 대비해 방문한 URL의 목록이 디스크에 저장되었는지 확인한다
  - `파티셔닝`
    - 웹이 성장하면서 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것은 불가능해졌다
    - 크롤링을 완수하기엔 한 대의 컴퓨터로는 메모리, 디스크 공간, 연산 능력, 네트워크 대역폭이 충분하지 못할 수도 있다
    - 몇몇 대규모 웹 로봇은 각각이 분리된 한 대의 컴퓨터인 로봇들이 동시에 일하고 있는 농장(farm)을 이용한다
    - 각 로봇엔 URL들의 특정 한 부분이 할당되어 그에 대한 책임을 진다
    - 로봇들은 서로 도와 웹을 크롤링한다
    - 개별 로봇들은 URL들을 이리저리 넘겨주거나 오동작하는 동료를 도와주거나 혹은 그 외의 이유로 그들의 활동을 조정하기 위해 커뮤니케이션을 할 필요가 있다

## 7. 별칭(alias)과 로봇순환(9.1.6)

- 올바른 자료구조를 갖추었더라도 URL이 별칭을 가질 수 있는 이상 어떤 페이지를 이전에 방문했었는지 말해주는 게 쉽지 않을 때도 있다
- `한 URL이 또 다른 URL에 대한 별칭이라면 그 둘이 서로 달라 보이더라도 사실은 같은 리소스를 가리키고 있다`
- 다음은 다른 URL들이 같은 리소스를 가리키게 되는 몇가지 간단한 예이다
  ![KakaoTalk_20230205_213639205_01.jpg](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/d02af739-3bc1-4a6f-84bf-4b91e68b8443/KakaoTalk_20230205_213639205_01.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230205%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230205T144334Z&X-Amz-Expires=86400&X-Amz-Signature=ece055ab59786c865c91747fe6c8c08b13dcf0fc01fe69ca77d1c96cf01e73ec&X-Amz-SignedHeaders=host&response-content-disposition=filename%3D%22KakaoTalk_20230205_213639205_01.jpg%22&x-id=GetObject)

## 8. URL 정규화하기(9.1.7)

- 대부분의 웹 로봇은 URL들을 표준 형식으로 `‘정규화’`함으로써 다른 URL과 같은 리소스를 가리키고 있음이 확실한 것들을 미리 제거하려 시도한다
- 로봇은 다음과 같은 방식으로 모든 URL을 정규화된 형식으로 변환할 수 있다

1. 포트 번호가 명시되지 않았다면 호스트명에 ‘:80’을 추가한다
2. 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다
3. # 태그들을 제거한다

- 이 단계들은 위의 그림이 a부터 c까지에 보여진 문제들을 제거할 수 있다
- 그러나 각 웹 서버에 대한 지식없이 로봇이 d에서 f까지에서와 같은 중복을 피할 수 있는 좋은 방법은 없다
  - 로봇은 이의 표의 d 문제를 피하기 위해 `웹 서버가 대소문자를 구분하는지` 알 필요가 있을 것
  - 로봇은 표 e의 URL들이 같은 리소스를 가리키는지 알려면 `이 디렉터리에 대한 웹 서버의 색인 페이지 설정을 알 필요가 있을 것`
  - 로봇이 표 f의 URL들이 `같은 리소스를 가리키는지 알려면 첫번째 URL의 호스트 명과 두번째 URL의 IP주소가 같은 물리적 컴퓨터를 참조한다는 것뿐 아니라 웹서버가 가상 호스팅을 하도록 설정되어 있는지`도 알아야 한다
- URL 정규화는 기본적인 문법의 별칭을 제거할 수 있지만 로봇들은 URL을 표준 형식으로 변환하는 것만으로는 제거할 수 없는 다른 URL별칭도 만나게 될 것이다

## 9. 파일 시스템 링크 순환(9.1.8)

- 심벌릭 링크 순환은 서버 관리자가 실수로 만들게 되는 것이 보통이지만 때때로 ‘사악한 웹 마스터’가 로봇을 함정에 빠뜨리기 위해 악의적으로 만들기도 한다
- 아래 그림은 두 파일 시스템을 보여주고 있다. subdir은 보통 디렉터리이다. subdir은 / 을 가리키는 심벌릭 링크다. 두 그림 모두에서 파일 /index.html은 파일 subdir/index.html 을 가리키는 하이퍼링크를 담고 있다

![그림](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/0d8b1313-0584-4639-937e-d68a079fac7b/KakaoTalk_20230205_233145835.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20230205%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20230205T144425Z&X-Amz-Expires=86400&X-Amz-Signature=e1492a99e3c1f299ca1ed75ee2ec5191fd9df2c639b0b874f626f36e2729c512&X-Amz-SignedHeaders=host&response-content-disposition=filename%3D%22KakaoTalk_20230205_233145835.jpg%22&x-id=GetObject)

- 9-3 a의 파일 시스템을 사용해 웹 크롤러는 다음의 동작을 수행할 것

1. GET [http://www.foo.com/index.html](http://www.foo.com/index.html)

- /index.html을 가져와서 subdir/index.html 로 이어지는 링크를 발견

1. GET http://www.foo.com/subdir/index.html

- subdir/index.html 을 가져와서 subdir/logo.gif 로 이어지는 링크를 발견

1. GET http://www.foo.com/subdir/logo.gif

- subdir/logo.gif 를 가져오고 더이상 링크가 없으므로 여기서 완료한다

그러나, 9-3b 의 파일 시스템에서는 다음과 같은 일이 일어났을 것이다

1. GET http://www.foo.com/index.html

- /index.html 을 가져와서 subdir/index.html 로 이어지는 링크를 발견한다

1. GET http://www.foo.com/subdir/index.html

- subdir/index.html을 가져왔지만 같은 index.html로 되돌아간다

1. GET http://www.foo.com/subdir/subdir/index.html

- subdir/subdir/index.html 을 가저온다

1. GET http://www.foo.com/subdir/subdir/subdir/index.html

- subdir/subdir/subdir/index.html을 가져온다

- 9-3b의 문제는 subdir/이 /로 링크되어 있기 때문에 순환되는 것이지만 URL이 달라 보이기 때문에 로봇은 URL만으로는 문서가 같다는 것을 모른다는 점이다
- 이상한 낌새를 눈치채지 못한 로봇은 루프로 빠져들 위험이 있다
- 어떤 식으로든 루프 발견을 하지 못한다면, URL의 길이가 로봇이나 서버의 한계를 넘을 때까지 이 순환은 반복될 것이다

## 10. 동적 가상 웹 공간

- 악의적인 웹 마스터들이 로봇들을 함정으로 빠뜨리기 위해 `의도적으로 복합한 크롤러 루프를 만드는 것은 있을 수 있는 일`
  - 특히 평범한 파일처럼 보이지만 사실은 게이트웨이 애플리케이션인 URL을 만드는 것은 쉬운일
- 이보다 더 흔하게 일어날 수 있는 일은 `웹 마스터가 나쁜 뜻이 없음에도 자신도 모르게 심벌릭 링크나 동적 콘텐츠를 통한 크롤러 함정을 만드는 것`
  - 예를 들어 한 달치 달력을 생성하고 그 다음 달로 링크를 걸어주는 CGI 기반의 달력 프로그램을 생각해보자. 실제 사용자라면 영원히 다음 달 링크만 누르고 있지는 않겠지만 `콘텐츠의 동적 성질을 이해하지 못하는 로봇이라면 무한히 다음 달 달력을 요청할 수 있다`

## 11. 루프와 중복 피하기(9.1.10)

- 모든 순환을 피하는 완벽한 방법은 없다
- 일반적으로 더욱 자율적인 크롤러(인간의 감독을 덜 필요로 하는)는 더 쉽게 곤란한 상황에 부딪힌다
- 로봇 구현자가 생각해야 할 트레이드오프가 하나 있는데 이들 휴리스틱은 문제를 피하는데 도움을 주지만 동시에 약간 ‘손실’을 유발할 수도 있다
- 웹은 로봇이 문제를 일으킬 가능성으로 가득차있기 때문에 이러한 웹에서 로봇이 더 올바르게 동작하기 위해 사용하는 기법들은 다음과 같다
  - `URL 정규화`
    - URL을 표준 형태로 변한함으로써 같은 리소스를 가리키는 중복된 URL이 생기는 것을 일부 회피
  - `너비 우선 크롤링`
    - 크롤러들은 언제든지 크롤링을 할 수 있는 URL들의 큰 집합을 갖고 있다
    - 방문할 URL들을 웹 사이트들 전체에 걸쳐 너비 우선으로 스케줄링하면 순환의 영향을 최소화할 수 있다
    - 혹여 로봇 함정을 건드리게 되더라도 여전히 그 순환에서 페이지를 받아오기 전에 다른 웹 사이트들에서 수십만개의 페이지들을 받아올 수 있다
    - 만약 로봇을 깊이 우선 방식으로 운용하여 웹 사이트 하나에 성급하게 뛰어든다면 순환을 건드리는 경우 영원히 다른 사이트로 빠져나올 수 없게 될 것이다
  - `스로틀링`
    - 로봇이 웹 사이트에서 일정 시간동안 가져올 수 있는 페이지의 숫자를 제한한다
    - 만약 로봇이 순환을 건드려서 지속적으로 그 사이트의 별칭들에 접근을 시도한다면 스로틀링을 이용해 그 서버에 대한 접근 횟수와 중복의 총 횟수를 제한할 수 있을 것이다
  - `URL 크기 제한`
    - 로봇은 일정 길이(보통 1KB)를 넘는 URL의 크롤링은 거부할 수 있다
    - 만약 순환으로 인해 URL이 계속해서 길어진다면 결국에는 길이 제한으로 인해 순환이 중단될 것이다
    - 어떤 웹 서버들은 긴 URL이 주어진 경우 실패한다
      - 그리고 URL이 점점 길어지는 순환에 빠진 로봇은 이러한 웹 서버들과 충돌을 유발할 수 있다
      - 이것은 웹 마스터가 로봇을 서비스 거부 공격자로 오해하도록 만든다
    - 주의해야 할 점은 이 기법을 적용하면 가져오지 못하는 콘텐츠들도 틀림없이 있을 것이라는 점이다
    - 많은 사이트들이 오늘날 URL을 사용자 상태를 관리하기 위해 사용한다
    - URL 길이는 크롤링을 제한하는 방법으로서는 까다로운 것일 수 있다
      - 그러나 이 기법은 요청 URL이 특정 크기에 도달할 떄마다 에러 로그를 남김으로써 특정 사이트에서 어떤 일이 벌어지는지 감시하는 사용자에게는 훌륭한 신호를 제공할 수 있다
  - `URL/사이트 블랙리스트`
    - 로봇 순환을 만들어내거나 함정인 것으로 알려진 사이트와 URL의 목록을 만들어 관리하고 그들을 전염병 피하듯 피한다
    - 문제를 일으키는 사이트나 URL이 발견될 때마다 이 블랙리스트에 추가한다
  - `패턴 발견`
    - 파일 시스템의 심벌릭 링크를 통한 순환과 그와 비슷한 오설정들은 일정 패턴을 따르는 경향이 있다
    - URL은 중복된 구성 요소와 함께 점점 길어질 수도 있다
    - 몇몇 로봇은 반복되는 구성요소를 가진 URL을 잠재적인 순환으로 보고 둘 혹은 셋 이상의 반복된 구성 요소를 갖고 있는 URL을 크롤링하는 것을 거절한다
  - `콘텐츠 지문`
    - 지문은 더욱 복잡한 웹 크롤러들 몇몇에 의해 사용되는 중복을 감지하는 보다 직접적인 방법이다
    - 콘텐츠 지문을 사용하는 로봇들은 페이지의 콘텐츠에서 몇 바이트를 얻어내어 체크섬을 계산한다
      - 이 체크섬은 그 페이지 내용의 간략한 표현이다
    - 만약 로봇이 이전에 보았던 체크섬을 가진 페이지를 가져온다면 그 페이지의 링크는 크롤링하지 않는다
    - 로봇이 그 페이지의 콘텐츠를 이전에 본 것이므로 그 페이지의 링크들은 이미 크롤링이 시작된 상태이기 때문이다
    - 체크섬 함수는 어떤 페이지가 서로 내용이 다름에도 체크섬은 똑같을 확률이 적은 것을 사용해야 한다
    - 지문 생성용으로는 MD5와 같은 메시지 요약 함수가 인기 있다
    - 어떤 웹 서버들은 동적으로 그때그때 페이지를 수정하기 때문에 로봇들은 때떄로 웹 페이지 콘텐츠에 임베딩된 링크와 같은 특정 부분들을 체크섬 계싼에서 빠드린다
    - 뿐만 아니라 페이지 콘텐츠를 임의로 커스터마이징하는 것을 포함한 서버 측의 동적인 동작은 중복감지를 방해할 수 있다
  - `사람의 모니터링`
    - 로봇은 결국 자신에게 적용된 어떤 기법으로도 해결할 수 없는 문제에 봉착하게 될 것이다
    - 모든 상용수준의 로봇은 사람이 쉽게 로봇의 진행 상황을 모니터링해서 뭔가 특이한 일이 일어나면 즉각 인지할 수 있게끔 반드시 진단과 로깅을 포함하도록 설계되어야 한다
    - 그렇지 않으면 화가 난 누리꾼들이 당신에게 보낸 험악한 이메일을 통해 문제를 인지하게 될지도 모른다
