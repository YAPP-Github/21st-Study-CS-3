## 9.4 로봇 차단하기
### robots.txt
- 로봇의 접근을 제어하는 정보를 담은 파일
- 로봇이 `robots.txt` 를 먼저 검사한 후 리소스에 접근
### 로봇 차단 표준
- v.0.0 / v.1.0 (대부분 1.0 표준에 맞추는 중)
### 웹 사이트와 robots.txt 파일들
- 웹 사이트의 어떤 URL을 방문하기 전에, 그 웹 사이트에 robots.txt파일이 존재한다면 로봇은 반드시 그 파일을 가져와서 처리
- 사이트 전체에 대해서, 단 1개만 존재
- 각각의 디렉토리에 대해서 robot.txt 파일을 만드는 방법 x

1) robots.txt 가져오기: HTTP GET메서드를 이용해 robots.txt 리소스를 가져온다. 
2) 응답 코드
- 200 코드 반환 (`text/plain` 반환 )
- 404 코드 반환(로봇의 접근에 대한 제한x)
- 401 혹은 403 코드 반환(로봇은 해당 사이트로의 접근을 아예 해서는 안된다.)
- 503 코드 반환 (일시적 실패 -> 일정 시간 후 다시 시도)
- 3XX 코드 반환 (리다이렉션 -> 로봇은 리소스가 발견될 때까지 리다이렉트를 따라간다)
```
GET /robots.txt HTTP/1.0
Host: www.joes-hardware.com
User-Agent: Slurp/2.0 //로봇의 접근을 추적하도록 신원 정보를 넘김
Date: Wed Oct 3 20:22:48 EST 2001
```

### robot.txt 파일 포맷
```
# slurp, webcrawler가 우리 사이트의 공개된 영역을 크롤링하는 것을 허용
User-Agent: slurp
User-Agent: webcrawler
Disallow: /private

User-Agent: *
Disallow:
```
- User-Agent 줄: 각 로봇의 이름(* 와일드 카드 사용 -> 모든 로봇에 대한 접근 ok)
- Disallow / Allow : 어떤 URL경로가 명시적으로 금지되어 있고 명시적으로 허용
- 이스케이핑 문자 : 원래 문자대로 적용 
-> ex) `%7E` == `~` == `"%7E"`
- 로봇은 자신이 이해하지 못하는 필드는 무시

### robots.txt의 캐싱과 만료
- 로봇은 매번 요청할 때마다 `robots.txt`를 새로 가져오는 것이 아니라 주기적으로 `robots.txt` 를 가져와서 그 결과를 캐시해야 한다.
- 응답에 존재하는 `Cache-control` `Expires` 헤더에 주의

### 로봇 차단 펄 코드
- robots.txt 파일과 상호작용하는 공개된 펄(Perl) 라이브러리
- CPAN 공개 펄 아카이브의 WWW::RobotsRules 모듈이다.
- [next-sitemap](https://github.com/iamvishnusankar/next-sitemap)

### HTML 로봇 제어 META 태그
- HTML 페이지 저자는 로봇이 개별 페이지에 접근하는 것을 제한하는 좀 더 직접적인 방법 -> 메타 태그 활용!
```js
<META NAME="ROBOTS" CONTENT="NOINDEX"> // 이 페이지를 처리하지 말고 무시
<META NAME="ROBOTS" CONTENT="NOFOLLOW"> // 이 페이지가 링크한 페이지를 무시
<META NAME="ROBOTS" CONTENT="INDEX"> // 이 페이지의 콘텐츠를 인덱싱해도 됨
<META NAME="ROBOTS" CONTENT="FOLLOW"> // 이 페이지가 링크한 페이지를 크롤링해도 됨
<META NAME="ROBOTS" CONTENT="NOARCHIVE"> // 이 페이지의 캐시를 만들어서는 안됨
<META NAME="ROBOTS" CONTENT="ALL"> // ALL=INDEX + FOLLOW   
<META NAME="ROBOTS" CONTENT="NONE"> // NOINDEX + NOFOLLOW
```
### 검색엔진 메타 태그
```js
<META NAME="description" CONTENT="사이트 소개 내용">
<META NAME="keywords" CONTENT="키워드1,키워드2,키워드3">
<META NAME="revisit-after" CONTENT="10 days"> //로봇이나 검색 엔진에게 지정된 날짜 이후에 다시 한번 방문하라는 의미
```